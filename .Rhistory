else
SIP1$SPAM[i] <- 0
}
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
text<- SIP1$ContentSummary
class(text)
text<-as.String(text)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann <- Maxent_POS_Tag_Annotator()
text_annotations <- annotate(text, list(sent_ann, word_ann))
text_annotations <- annotate(text, list(sent_ann, word_ann))
text
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
pos_ann <- Maxent_POS_Tag_Annotator()
text_annotations <- annotate(text, list(sent_ann, word_ann))
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("NLP")
library("RWeka")
library("openNLP")
SIP<-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
SIP1 <- SIP[,c(1,6,28)]
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
text<- SIP1$ContentSummary
class(text)
text<-as.String(text)
text
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
text_annotations <- annotate(text, list(sent_ann, word_ann))
SIP1 <- as.data.frame(SIP1)
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
text<- SIP1$ContentSummary
class(text)
text<-as.String(text)
text
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
text_annotations <- annotate(text, list(sent_ann, word_ann))
class(text)
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("NLP")
library("RWeka")
library("openNLP")
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
a1 <- SIP[,c(1,6,28)]
a2 <- as.String(SIP$ContentSummary)
str(a1)
a1 <- a[,c(1,6,28)]
a2 <- as.String(SIP$ContentSummary)
a2 <- as.String(a$ContentSummary)
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
ax <- a[,c(1,6,28)]
ax <- as.String(ax$ContentSummary)
s <- a[,c(1,6,28)]
s <- as.String(s$ContentSummary)
s <- as.String(s)
## Need sentence and word token annotations.
## Need sentence and word token annotations.
s <- as.String(s)
## Need sentence and word token annotations.sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
pos_tag_annotator
a3 <- annotate(s, pos_tag_annotator, a2)
a3
## Variant with POS tag probabilities as (additional) features.
head(annotate(s, Maxent_POS_Tag_Annotator(probs = TRUE), a2))
## Determine the distribution of POS tags for word tokens.
a3w <- subset(a3, type == "word")
tags <- sapply(a3w$features, `[[`, "POS")
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("NLP")
library("RWeka")
library("openNLP")
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
s <- a[,c(1,6,28)]
s <- as.String(s$ContentSummary)
s <- as.String(s)
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library(NLP)
library(openNLP)
library(RWeka)
library(NLP)
library(openNLP)
library(magrittr)
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
s <- a[,c(1,6,28)]
s <- as.String(s$ContentSummary)
s <- as.String(s)
word_token_annotator <- Maxent_Word_Token_Annotator()
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library(NLP)
library(openNLP)
library(RWeka)
library(NLP)
library(openNLP)
library(magrittr)
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
s <- a[,c(1,6,28)]
bio <- as.String(s$ContentSummary)
bio <- as.String(bio)
## Need sentence and word token annotations.
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
word_annotations <- annotate(s, list(sent_ann, word_ann))
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library(NLP)
library(openNLP)
library(RWeka)
library(NLP)
library(openNLP)
library(magrittr)
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
s <- a[,c(1,6,28)]
bio <- as.String(s$ContentSummary)
bio <- as.String(bio)
## Need sentence and word token annotations.
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
word_annotations <- annotate(s, list(sent_ann, word_ann))
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("RWeka")
library("openNLP")
SIP<-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
SIP1 <- SIP[,c(1,6,28)]
# SIP1$SPAM <- 0
Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(ContentSummary, list(sent_token_annotator, word_token_annotator))
sentence <- SIP1$ContentSummary[1]
a<-tagPOS(sentence, language = "en")
nrow(SIP1)
grepl("http", SIP1$ContentSummary[1])
# SIP1 <- transform(SIP1, SIP1$SPAM=apply(SIP1, 2, function(x) grepl("http", SIP1$ContentSummary)))
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
warnings()
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
library(tm)
library(SnowballC)
Sys.setlocale('LC_ALL','C')
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(ContentSummary, list(sent_token_annotator, word_token_annotator))
sentence <- SIP1$ContentSummary[1]
a<-tagPOS(sentence, language = "en")
nrow(SIP1)
grepl("http", SIP1$ContentSummary[1])
# SIP1 <- transform(SIP1, SIP1$SPAM=apply(SIP1, 2, function(x) grepl("http", SIP1$ContentSummary)))
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
warnings()
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
library(tm)
library(SnowballC)
Sys.setlocale('LC_ALL','C')
corpusposts <- VCorpus(VectorSource(SIP2$ContentSummary))
inspect(corpusposts)
corpusposts <- tm_map(corpusposts, removeNumbers, lazy=TRUE)
corpusposts <- tm_map(corpusposts, removePunctuation, lazy=TRUE)
corpusposts <- tm_map(corpusposts, removeWords, stopwords("english"), lazy=TRUE)
corpusposts <- tm_map(corpusposts, stripWhitespace, lazy=TRUE)
corpusposts <- tm_map(corpusposts, PlainTextDocument, lazy=TRUE)
myDtm <- t(TermDocumentMatrix(corpusposts, control = list(minWordLength = 1)))
library(slam)
colTotals <-  col_sums(myDtm)
colTotals
myDtm1 <- myDtm[,which(colTotals > 5)]
myDtm1
a2 <- annotate(ContentSummary, list(sent_token_annotator, word_token_annotator))
bio <- as.data.frame(bio)
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library(NLP)
library(openNLP)
library(RWeka)
library(NLP)
library(openNLP)
library(magrittr)
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
s <- a[,c(1,6,28)]
bio <- as.String(s$ContentSummary)
bio <- as.String(bio)
bio <- as.data.frame(bio)
bio1 <- as.data.frame(bio)
bio <- as.String(s$ContentSummary)
# bio <- as.String(bio)
bio1 <- as.data.frame(bio)
bio <- as.String(s$ContentSummary)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
word_annotations <- annotate(s, list(sent_ann, word_ann))
bio <- as.data.frame(s$ContentSummary)
# bio <- as.String(bio)
# bio1 <- as.data.frame(bio)
## Need sentence and word token annotations.
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
word_annotations <- annotate(s, list(sent_ann, word_ann))
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library(NLP)
library(openNLP)
library(RWeka)
library(NLP)
library(openNLP)
library(magrittr)
a <-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
s <- a[,c(1,6,28)]
bio <- as.data.frame(s$ContentSummary)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
word_annotations <- annotate(s, list(sent_ann, word_ann))
detach("package:datasets", unload=TRUE)
detach("package:caret", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
detach("package:graphics", unload=TRUE)
detach("package:grDevices", unload=TRUE)
detach("package:klaR", unload=TRUE)
detach("package:lattice", unload=TRUE)
detach("package:MASS", unload=TRUE)
detach("package:NLP", unload=TRUE)
detach("package:openNLP", unload=TRUE)
detach("package:RWeka", unload=TRUE)
detach("package:slam", unload=TRUE)
detach("package:SnowballC", unload=TRUE)
detach("package:stats", unload=TRUE)
detach("package:tm", unload=TRUE)
detach("package:utils", unload=TRUE)
detach("package:methods", unload=TRUE)
detach("package:magrittr", unload=TRUE)
detach("package:e1071", unload=TRUE)
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("NLP")
library("RWeka")
library("openNLP")
install.packages("javar")
install.packages("rJava")
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("NLP")
library("RWeka")
library("openNLP")
SIP<-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
SIP1 <- SIP[,c(1,6,28)]
SIP1 <- as.data.frame(SIP1)
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
text<- SIP1$ContentSummary
class(text)
text<-as.String(text)
class(text)
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
text_annotations <- annotate(text, list(sent_ann, word_ann))
pos_ann <- Maxent_POS_Tag_Annotator()
class(text_annotations)
text_doc <- AnnotatedPlainTextDocument(text, text_annotations)
sents(text_doc) %>% head(2)
words(text_doc) %>% head(10)
person_ann <- Maxent_Entity_Annotator(kind = "person")
location_ann <- Maxent_Entity_Annotator(kind = "location")
organization_ann <- Maxent_Entity_Annotator(kind = "organization")
pipeline <- list(sent_ann,
word_ann,
person_ann,
location_ann,
organization_ann)
text_annotations <- annotate(text, pipeline)
text_doc <- AnnotatedPlainTextDocument(text, text_annotations)
# Extract entities from an AnnotatedPlainTextDocument
entities <- function(doc, kind) {
s <- doc$content
a <- annotations(doc)[[1]]
if(hasArg(kind)) {
k <- sapply(a$features, `[[`, "kind")
s[a[k == kind]]
} else {
s[a[a$type == "entity"]]
}
}
entities(text_doc, kind = "person")
entities(text_doc, kind = "location")
entities(text_doc, kind = "organization")
Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)
text_pos <- annotate(text, pos_ann, text_annotations)
text_pos
## Variant with POS tag probabilities as (additional) features.
head(annotate(text, Maxent_POS_Tag_Annotator(probs = TRUE), text_annotations))
## Determine the distribution of POS tags for word tokens.
text_postag <- subset(text_pos, type == "word")
tags <- sapply(text_postag$features, `[[`, "POS")
tags
table(tags)
library(RSQLite)
require(dplyr)
library(wordcloud)
library(tm)
library(RColorBrewer)
sub <- "borrow"
max_words <- 75
db <- src_sqlite('../input/database.sqlite', create = F)
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("RWeka")
library("openNLP")
SIP<-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
SIP1 <- SIP[,c(1,6,28)]
# SIP1$SPAM <- 0
Maxent_POS_Tag_Annotator(language = "en", probs = FALSE, model = NULL)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(ContentSummary, list(sent_token_annotator, word_token_annotator))
a2 <- annotate(SIP1$ContentSummary, list(sent_token_annotator, word_token_annotator))
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
warnings()
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
library(tm)
library(SnowballC)
Sys.setlocale('LC_ALL','C')
corpusposts <- VCorpus(VectorSource(SIP2$ContentSummary))
inspect(corpusposts)
corpusposts <- tm_map(corpusposts, removeNumbers, lazy=TRUE)
corpusposts <- tm_map(corpusposts, removePunctuation, lazy=TRUE)
corpusposts <- tm_map(corpusposts, removeWords, stopwords("english"), lazy=TRUE)
corpusposts <- tm_map(corpusposts, stripWhitespace, lazy=TRUE)
corpusposts <- tm_map(corpusposts, PlainTextDocument, lazy=TRUE)
dtm <- DocumentTermMatrix(corpusposts,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
dtm_sparse <- removeSparseTerms(dtm, 0.995)
post_words <- as.data.frame(as.matrix(dtm_sparse))
total_words <- data.frame(words = colnames(post_words),
counts = colSums(post_words))
wordcloud(words = total_words$words,
freq=total_words$counts,
max.words = max_words,
color = brewer.pal(8,"Dark2"))
print(total_words$words)
library(wordcloud)
library(RColorBrewer)
wordcloud(words = total_words$words,
freq=total_words$counts,
max.words = max_words,
color = brewer.pal(8,"Dark2"))
wordcloud(words = total_words$words,
freq=total_words$counts,
color = brewer.pal(8,"Dark2"))
max_words <- 75
wordcloud(words = total_words$words,
freq=total_words$counts,
max.words = max_words,
color = brewer.pal(8,"Dark2"))
print(total_words$words)
dtm
dtm_sparse
post_words
total_words <- data.frame(words = colnames(post_words),
post_words <- as.data.frame(as.matrix(dtm_sparse))
post_words
total_words <- data.frame(words = colnames(post_words),
counts = colSums(post_words))
library(wordcloud)
rm(list=ls(all=TRUE))
getwd()
setwd("/Users/ravan/R")
library("e1071")
library("caret")
library("klaR")
library("RWeka")
library("openNLP")
SIP<-read.csv("SIP_Sampx.csv",stringsAsFactors =F)
SIP1 <- SIP[,c(1,6,28)]
for(i in 1:nrow(SIP1)){
if(grepl("http", SIP1$ContentSummary[i]) == T)
SIP1$SPAM[i] <- 1
else
SIP1$SPAM[i] <- 0
}
warnings()
SIP2 <- subset(SIP1,SIP1$SPAM==0,-c(SPAM))
str(SIP2)
library(tm)
library(SnowballC)
Sys.setlocale('LC_ALL','C')
corpusposts <- VCorpus(VectorSource(SIP2$ContentSummary))
inspect(corpusposts)
corpusposts <- tm_map(corpusposts, removeNumbers, lazy=TRUE)
corpusposts <- tm_map(corpusposts, removePunctuation, lazy=TRUE)
corpusposts <- tm_map(corpusposts, removeWords, stopwords("english"), lazy=TRUE)
corpusposts <- tm_map(corpusposts, stripWhitespace, lazy=TRUE)
corpusposts <- tm_map(corpusposts, PlainTextDocument, lazy=TRUE)
# myDtm <- t(TermDocumentMatrix(corpusposts, control = list(minWordLength = 1)))
dtm <- DocumentTermMatrix(corpusposts,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE), stopwords = TRUE))
dtm_sparse <- removeSparseTerms(dtm, 0.995)
post_words <- as.data.frame(as.matrix(dtm_sparse))
post_words
total_words <- data.frame(words = colnames(post_words),
counts = colSums(post_words))
library(wordcloud)
library(RColorBrewer)
max_words <- 75
wordcloud(words = total_words$words,
freq=total_words$counts,
max.words = max_words,
color = brewer.pal(8,"Dark2"))
print(total_words$words)
source('~/R/NB Classifier.R')
library("openNLP")
acq <- "Gulf Applied Technologies Inc said it sold its subsidiaries engaged in
pipeline and terminal operations for 12.2 mln dlrs. The company said
the sale is subject to certain post closing adjustments,
which it did not explain. Reuter."
acqTag <- tagPOS(acq)
?tagPOS
??tagPOS
source('~/R/POStag.R')
